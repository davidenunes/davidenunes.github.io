---
title: Semantic Grounding with Incremental Neural Representation Learning
layout: splash
share: false
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: /assets/images/glitch.gif
  #actions:
  #  - label: "Download"
  #    url: "/"
#excerpt: "summary goes here"
---

{% include feature_row id="intro" type="center" %}

## Introduction

### Incremental Semantic Grounding
### Meaning, Language and Grounding
### Computational Semantics
### Representation Learning as Grounding
### Neural Networks and Probabilistic Modelling
### Distributed Representations and Random Projections
### Research Questions and Contributions

## Background and Related Work
### Distributional Semantics and Language Modelling
### Vector Space Models
### Dimensionality Reduction and Random Projections
### Neural Language Models
### Energy-based models and contrastive learning

## Neural Random Projections

### Neural Probabilistic Language Modelling
### Computational Bottlenecks and Unnormalized Models
### Random Projections
### Experimental Setup
### Model Performance and Compression
### Conclusion

## Compressive Predictions

### Maximum Likelihood Bottleneck
### Estimating Unnormalized Neural Language Models 
### Contrastive Estimation and Optimal Transport
### Experimental Setup
### Noise Contrastive Estimation vs Partial Optimal Transport
### Conclusion and Future Prospects

## Conclusion

### Summary of Contributions
### Challenges and Open Questions
### Compression as a Theory of the Mind
### Future Prospects in Structured Inference








